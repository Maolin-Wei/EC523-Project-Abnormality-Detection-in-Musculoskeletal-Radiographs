{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yKNoL8KN8vox"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WV1ia7DF9Mb7"},"outputs":[],"source":["# cd /content/drive/MyDrive/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2303,"status":"ok","timestamp":1710428442533,"user":{"displayName":"Maolin Wei","userId":"18104927944743564435"},"user_tz":240},"id":"hxi6xUay8mVf","outputId":"10d75729-270a-4b06-cc93-6e8045d8d741"},"outputs":[{"data":{"text/plain":["(  MURA-v1.1/train/XR_SHOULDER/patient00001/study1_positive/  1\n"," 0  MURA-v1.1/train/XR_SHOULDER/patient00002/study...         1\n"," 1  MURA-v1.1/train/XR_SHOULDER/patient00003/study...         1\n"," 2  MURA-v1.1/train/XR_SHOULDER/patient00004/study...         1\n"," 3  MURA-v1.1/train/XR_SHOULDER/patient00005/study...         1\n"," 4  MURA-v1.1/train/XR_SHOULDER/patient00006/study...         1,\n","   MURA-v1.1/valid/XR_WRIST/patient11185/study1_positive/  1\n"," 0  MURA-v1.1/valid/XR_WRIST/patient11186/study1_p...      1\n"," 1  MURA-v1.1/valid/XR_WRIST/patient11186/study2_p...      1\n"," 2  MURA-v1.1/valid/XR_WRIST/patient11186/study3_p...      1\n"," 3  MURA-v1.1/valid/XR_WRIST/patient11187/study1_p...      1\n"," 4  MURA-v1.1/valid/XR_WRIST/patient11188/study1_p...      1)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","train_df = pd.read_csv('MURA-v1.1/train_labeled_studies.csv')\n","valid_df = pd.read_csv('MURA-v1.1/valid_labeled_studies.csv')\n","\n","train_df.head(), valid_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":688,"status":"ok","timestamp":1710428443218,"user":{"displayName":"Maolin Wei","userId":"18104927944743564435"},"user_tz":240},"id":"uBLkOAlS8mVk","outputId":"fd567594-2a5f-4d84-eeb9-77b47ab23685"},"outputs":[{"name":"stderr","output_type":"stream","text":["/scratch/5671805.1.csgpu/ipykernel_1034708/957062800.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  image_path = row[0]  # Assuming the first column contains the image paths\n","/scratch/5671805.1.csgpu/ipykernel_1034708/957062800.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  label = row[1]       # Assuming the second column contains the labels\n"]},{"data":{"text/plain":["(13456,\n"," 1198,\n"," [('MURA-v1.1/train/XR_SHOULDER/patient00002/study1_positive/', 1),\n","  ('MURA-v1.1/train/XR_SHOULDER/patient00003/study1_positive/', 1),\n","  ('MURA-v1.1/train/XR_SHOULDER/patient00004/study1_positive/', 1),\n","  ('MURA-v1.1/train/XR_SHOULDER/patient00005/study1_positive/', 1),\n","  ('MURA-v1.1/train/XR_SHOULDER/patient00006/study1_positive/', 1)],\n"," [('MURA-v1.1/valid/XR_WRIST/patient11186/study1_positive/', 1),\n","  ('MURA-v1.1/valid/XR_WRIST/patient11186/study2_positive/', 1),\n","  ('MURA-v1.1/valid/XR_WRIST/patient11186/study3_positive/', 1),\n","  ('MURA-v1.1/valid/XR_WRIST/patient11187/study1_positive/', 1),\n","  ('MURA-v1.1/valid/XR_WRIST/patient11188/study1_positive/', 1)])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["def prepare_data(df):\n","    data = []\n","    for index, row in df.iterrows():\n","        image_path = row[0]  # Assuming the first column contains the image paths\n","        label = row[1]       # Assuming the second column contains the labels\n","        data.append((image_path, label))\n","    return data\n","\n","train_data = prepare_data(train_df)\n","valid_data = prepare_data(valid_df)\n","\n","len(train_data), len(valid_data), train_data[:5], valid_data[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1710428443218,"user":{"displayName":"Maolin Wei","userId":"18104927944743564435"},"user_tz":240},"id":"tUhOzyV_8mVm","outputId":"71c9869a-c510-40d9-f526-cfa7dafe2572"},"outputs":[{"data":{"text/plain":["({'XR_SHOULDER': 2820,\n","  'XR_HUMERUS': 592,\n","  'XR_FINGER': 1935,\n","  'XR_ELBOW': 1754,\n","  'XR_WRIST': 3460,\n","  'XR_FOREARM': 877,\n","  'XR_HAND': 2018},\n"," {'XR_WRIST': 236,\n","  'XR_FOREARM': 133,\n","  'XR_HAND': 167,\n","  'XR_HUMERUS': 135,\n","  'XR_SHOULDER': 194,\n","  'XR_ELBOW': 158,\n","  'XR_FINGER': 175})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from collections import defaultdict\n","\n","def divide_by_body_part(data):\n","    body_parts_data = defaultdict(list)\n","    for path, label in data:\n","        body_part = path.split('/')[2]  # Assuming the third component is the body part\n","        body_parts_data[body_part].append((path, label))\n","    return body_parts_data\n","\n","train_data_by_body_part = divide_by_body_part(train_data)\n","valid_data_by_body_part = divide_by_body_part(valid_data)\n","\n","train_body_parts = {part: len(train_data_by_body_part[part]) for part in train_data_by_body_part}\n","valid_body_parts = {part: len(valid_data_by_body_part[part]) for part in valid_data_by_body_part}\n","\n","train_body_parts, valid_body_parts"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":171,"status":"ok","timestamp":1710428444070,"user":{"displayName":"Maolin Wei","userId":"18104927944743564435"},"user_tz":240},"id":"8ca1yzsp8mVn","outputId":"d5b79e50-f849-424e-bbbb-feec27a3ccad"},"outputs":[{"data":{"text/plain":["(                                          Image_Path  Label\n"," 0  MURA-v1.1/train/XR_SHOULDER/patient00002/study...      1\n"," 1  MURA-v1.1/train/XR_SHOULDER/patient00003/study...      1\n"," 2  MURA-v1.1/train/XR_SHOULDER/patient00004/study...      1\n"," 3  MURA-v1.1/train/XR_SHOULDER/patient00005/study...      1\n"," 4  MURA-v1.1/train/XR_SHOULDER/patient00006/study...      1,\n","                                           Image_Path  Label\n"," 0  MURA-v1.1/valid/XR_SHOULDER/patient11676/study...      1\n"," 1  MURA-v1.1/valid/XR_SHOULDER/patient11703/study...      1\n"," 2  MURA-v1.1/valid/XR_SHOULDER/patient11344/study...      1\n"," 3  MURA-v1.1/valid/XR_SHOULDER/patient11281/study...      1\n"," 4  MURA-v1.1/valid/XR_SHOULDER/patient11368/study...      1)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["train = {}\n","valid = {}\n","\n","def create_dfs_by_body_part(data_by_body_part, dfs_by_body_part):\n","    for body_part, data in data_by_body_part.items():\n","        df = pd.DataFrame(data, columns=['Image_Path', 'Label'])\n","        dfs_by_body_part[body_part] = df\n","\n","create_dfs_by_body_part(train_data_by_body_part, train)\n","create_dfs_by_body_part(valid_data_by_body_part, valid)\n","\n","train['XR_SHOULDER'].head(), valid['XR_SHOULDER'].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WOpLe2zX9snK"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from torchvision import transforms, models\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.datasets import ImageFolder\n","import os\n","\n","torch.manual_seed(42)\n","\n","class MURADataset(Dataset):\n","    def __init__(self, data_dict, transform=None):\n","        self.transform = transform\n","        self.data_list = []\n","\n","        for body_part, data in data_dict.items():\n","            for _, row in data.iterrows():\n","                img_dir = os.path.dirname(row['Image_Path'])\n","                for img_file in os.listdir(img_dir):\n","                    if img_file.startswith('.'):\n","                        continue\n","                    img_path = os.path.join(img_dir, img_file)\n","                    if os.path.isfile(img_path):\n","                        # Include body part information in the data_list\n","                        self.data_list.append((img_path, row['Label'], body_part))\n","\n","    def __len__(self):\n","        return len(self.data_list)\n","\n","    def __getitem__(self, idx):\n","        img_path, label, body_part = self.data_list[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label, body_part\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vb0XQ742FBL_"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, auc, confusion_matrix\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","import time\n","\n","def train_model(model, train_loader, criterion, optimizer, device, model_path, num_epochs=10):\n","    model.train()\n","\n","    # Lists to keep track of progress\n","    epoch_loss_history = []\n","    epoch_acc_history = []\n","    start_time = time.time()\n","\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_corrects = 0\n","        total = 0\n","\n","        for inputs, labels, _ in tqdm(train_loader):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            with torch.set_grad_enabled(True):\n","                outputs = model(inputs)\n","                _, preds = torch.max(outputs, 1)\n","                loss = criterion(outputs, labels)\n","\n","                loss.backward()\n","                optimizer.step()\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            running_corrects += torch.sum(preds == labels.data)\n","            total += labels.size(0)\n","\n","        epoch_loss = running_loss / total\n","        epoch_acc = running_corrects.double() / total\n","\n","        epoch_loss_history.append(epoch_loss)\n","        epoch_acc_history.append(epoch_acc.item())\n","\n","        print(f'Epoch {epoch + 1}/{num_epochs} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n","        if epoch % 20 == 0:\n","            torch.save(model.state_dict(), f'{model_path}_epoch_{epoch}_model.pth')\n","\n","    elapsed_time = time.time() - start_time\n","    return model, epoch_loss_history, epoch_acc_history, elapsed_time\n","\n","\n","def validate_model(model, valid_loader, device, model_path):\n","    model.eval()  # Set model to evaluate mode\n","\n","    all_preds = []\n","    all_labels = []\n","    parts_list = []  # To store body part information for each prediction\n","\n","    with torch.no_grad():\n","        for inputs, labels, parts in tqdm(valid_loader):  # Assuming body part info is available\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","            parts_list.extend(parts)  # Assuming 'parts' is a list or similar iterable\n","    #Calculate Values for Sensitivity and Specificity graph\n","    fpr, tpr, thresholds = roc_curve(all_labels, all_preds)\n","    roc_auc = auc(fpr, tpr)\n","    plot_roc_curve(fpr, tpr, roc_auc, model_path)\n","    # Calculate metrics for each body part\n","    parts_set = set(parts_list)  # Get unique body parts\n","    body_part_metrics = {}\n","\n","    for part in parts_set:\n","        part_indices = [i for i, p in enumerate(parts_list) if p == part]  # Indices of this part\n","        part_preds = np.array([all_preds[i] for i in part_indices])\n","        part_labels = np.array([all_labels[i] for i in part_indices])\n","\n","        tn, fp, fn, tp = confusion_matrix(part_labels, part_preds).ravel()\n","\n","        sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n","        specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n","        accuracy = (tp + tn) / (tp + tn + fp + fn)\n","\n","        body_part_metrics[part] = {\n","            'sensitivity': round(sensitivity, 2),\n","            'specificity': round(specificity, 2),\n","            'accuracy': round(accuracy, 2)\n","        }\n","\n","    # Calculate overall metrics\n","    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n","    overall_sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n","    overall_specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n","    overall_accuracy = (tp + tn) / (tp + tn + fp + fn)\n","\n","    # Prepare data to save\n","    metrics_data = [\n","        ['Body Part', 'Sensitivity', 'Specificity', 'Accuracy']\n","    ]\n","\n","    for part, metrics in body_part_metrics.items():\n","        metrics_data.append([part, metrics['sensitivity'], metrics['specificity'], metrics['accuracy']])\n","\n","    metrics_data.append(['Overall', round(overall_sensitivity, 2), round(overall_specificity, 2), round(overall_accuracy, 2)])\n","\n","    metrics_data.append(['fpr', 'tpr', 'thresholds', 'roc_auc'])\n","\n","    for i in range(len(fpr)):\n","        metrics_data.append([fpr[i], tpr[i], thresholds[i], roc_auc])\n","\n","\n","    return metrics_data\n","\n","\n","def plot_training_history(epoch_loss_history, epoch_acc_history):\n","    plt.figure(figsize=(12, 5))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epoch_loss_history, label='Loss')\n","    plt.title('Loss vs. Epoch')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.xticks(range(len(epoch_loss_history)))\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    print('epoch_acc_history:', epoch_acc_history)\n","    plt.plot(epoch_acc_history, label='Accuracy')\n","\n","    plt.title('Accuracy vs. Epoch')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.xticks(range(len(epoch_acc_history)))\n","    plt.legend()\n","\n","    plt.show()\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, model_path):\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic')\n","    plt.legend(loc=\"lower right\")\n","    plt.savefig(f'{model_path}_roc_curve.png')\n","    plt.close()  # Close the plot to avoid overlapping plots\n","    # plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zTPxXvj7_hF2"},"outputs":[],"source":["#Define Hyperparameters\n","# SAVE_PATH = '/content/drive/MyDrive/EC523_Project/Models/Model_Architectures'\n","SAVE_PATH = './Models/Model_Architectures'\n","learning_rate = 0.0001\n","momentum = 0.9\n","batch_size = 8\n","num_epochs = 100\n","crit = \"CrossEntropy\"\n","opti = \"SGD\"\n","transformations = \"ReSize: 320X320, Rotation: 30deg, Vert_Flip: 50%, Hor_Flip: 50%, Normalization:(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\"\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define a transformation pipeline with normalization\n","train_transform = transforms.Compose([\n","    transforms.Resize((320, 320)),\n","    transforms.ToTensor(),\n","    transforms.RandomRotation(30),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomVerticalFlip(p=0.5),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","valid_transform = transforms.Compose([\n","    transforms.Resize((320, 320)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","# Initialize the pretrained list of models\n","\n","models_list = [models.densenet121, models.densenet161, models.densenet169, models.densenet201,\n","               models.resnext101_32x8d, models.resnext101_64x4d, models.resnet152, models.resnet50]\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","train_dataset = MURADataset(train, transform=train_transform)\n","valid_dataset = MURADataset(valid, transform=valid_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers = 10)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers = 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nj8RrNUN03JA"},"outputs":[],"source":["import csv\n","\n","def save_history_to_csv(model_name, train_loss_history, train_acc_history, path):\n","    with open(path, mode='w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['Epoch', 'Train Loss', 'Train Accuracy'])\n","        for epoch, (loss, acc) in enumerate(zip(train_loss_history, train_acc_history), 1):\n","            writer.writerow([epoch, loss, acc])\n","\n","def save_model_training_parameters_to_csv(model_name,transformations,optimizer,momentum,batch_size,num_epochs, elapsed_time, path):\n","  with open(path, mode='w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['Model Name', 'Transformations', 'Optimizer', 'Learning Rate', 'Momentum', 'Batch Size', 'Number of Epochs', 'Training Time'])\n","        writer.writerow([model_name, transformations, optimizer, learning_rate, momentum, batch_size, num_epochs, elapsed_time])\n","\n","def save_validation_to_csv(metrics_data, path):\n","    with open(path, 'w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerows(metrics_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IePeN38H0-VL"},"outputs":[],"source":["for model_fn in models_list:\n","    model_name = model_fn.__name__\n","    model = model_fn(pretrained=True)\n","    model = model.to(device)\n","\n","    # Create paths for csv files\n","    model_folder = os.path.join(SAVE_PATH, model_name)\n","    model_path = os.path.join(model_folder, model_name)\n","\n","    if not os.path.exists(model_folder):\n","        os.makedirs(model_folder)\n","    if not os.path.exists(model_path):\n","        os.makedirs(model_path)\n","\n","    train_path = os.path.join(model_folder, 'train_history.csv')\n","    train_parameters_path = os.path.join(model_folder, 'train_parameters.csv')\n","    model_results_path = os.path.join(model_folder, 'validation_results.csv')\n","\n","    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n","    model, train_loss_history, train_acc_history,elapsed_time = train_model(model, train_loader, criterion, optimizer, device, model_path, num_epochs=num_epochs)\n","    metrics_data = validate_model(model, valid_loader, device, model_path)\n","\n","    # Save training results to CSV to their respective paths\n","    torch.save(model.state_dict(), f'{model_path}_final_model.pth')\n","    save_history_to_csv(model_name, train_loss_history, train_acc_history,train_path)\n","    save_model_training_parameters_to_csv(model_name,transformations,opti,momentum,batch_size,num_epochs,elapsed_time, train_parameters_path)\n","    save_validation_to_csv(metrics_data, model_results_path)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}